{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Yi Wen/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'vgg11', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data: if Windows\n",
    "path='../Image/All_Cropped'\n",
    "\n",
    "images={}\n",
    "\n",
    "directory=['Mountain', 'Beach', \n",
    "           'Mug', 'Banana', \n",
    "           'Car', 'Plane', \n",
    "           'Lighthouse', 'Church']\n",
    "\n",
    "for dir in directory:\n",
    "    images[dir]=[file for file in os.listdir(path+'/'+dir) if file.endswith(('jpeg', 'jpg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mountain : 16\n",
      "Beach : 16\n",
      "Mug : 16\n",
      "Banana : 16\n",
      "Car : 16\n",
      "Plane : 16\n",
      "Lighthouse : 16\n",
      "Church : 16\n"
     ]
    }
   ],
   "source": [
    "for key in images.keys():\n",
    "    print(key, \":\", len(images[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels \n",
    "with open(\"../imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# read labels to wordnet synsets\n",
    "ltw = pd.read_json('../imagenet_label_to_wordnet_synset.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'label', 'uri'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean and std of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "all_images = []\n",
    "for cate, filename in images.items():\n",
    "    for f in filename:\n",
    "        dir=os.path.join(path,cate,f)\n",
    "        img=preprocess(Image.open(dir))\n",
    "#         if img.shape[1] != 256 or img.shape[2] != 256:\n",
    "#             print(img.shape)\n",
    "#             plt.imshow(Image.open(dir))\n",
    "#             plt.show()\n",
    "        all_images.append(img)\n",
    "all_images = torch.stack(all_images).numpy()\n",
    "print(all_images.shape)\n",
    "mean = np.mean(all_images, axis=(0,2,3))\n",
    "std = np.std(all_images, axis=(0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55640996, 0.5760121 , 0.56940985], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3011326 , 0.28208593, 0.31378052], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=mean, std=std),\n",
    "# ])\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type prob cate+filename\n",
    "all_img={'type':[], 'dir':[],'id_labels':[],'readable_labels':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels \n",
    "with open(\"../imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cate, filename in images.items():\n",
    "    for f in filename:\n",
    "        dir=os.path.join(path,cate,f)\n",
    "        img=Image.open(dir)\n",
    "        #print(dir)\n",
    "        input_tensor=preprocess(img)\n",
    "        input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            input_batch= input_batch.to('cuda')\n",
    "            model.to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        top10_prob, top10_catid = torch.topk(probabilities, 10)\n",
    "#         typ_score=round(float(probabilities.max()*100),1)\n",
    "        \n",
    "        # Show top categories per image\n",
    "        top10_prob, top10_catid = torch.topk(probabilities, 10)\n",
    "        temp1 = {}\n",
    "        temp2 = {}\n",
    "        for i in range(top10_prob.size(0)):\n",
    "            temp1[categories[top10_catid[i].item()]] = top10_prob[i].item()\n",
    "            temp2[ltw['id'][top10_catid[i].item()]] = top10_prob[i].item()\n",
    "\n",
    "        all_img['dir'].append(dir)\n",
    "#         all_img['typicality_score'].append(typ_score)\n",
    "        all_img['readable_labels'].append(temp1)\n",
    "        all_img['id_labels'].append(temp2)\n",
    "        if 'Mountain' in cate:\n",
    "            all_img['type'].append('Mountain')\n",
    "        if 'Beach' in cate:\n",
    "            all_img['type'].append('Beach')\n",
    "        if 'Mug' in cate:\n",
    "            all_img['type'].append('Mug')\n",
    "        if 'Banana' in cate:\n",
    "            all_img['type'].append('Banana')\n",
    "        if 'Car' in cate:\n",
    "            all_img['type'].append('Car')\n",
    "        if 'Plane' in cate:\n",
    "            all_img['type'].append('Plane')\n",
    "        if 'Lighthouse' in cate:\n",
    "            all_img['type'].append('Lighthouse')\n",
    "        if 'Church' in cate:\n",
    "            all_img['type'].append('Church')\n",
    "\n",
    "#         print('Typicality of ',f, ' = ',probabilities.max()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_img_df=pd.DataFrame(all_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_img_df['label'] = all_img_df['top5_labels'].apply(lambda x: list(x.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_img_df['7_score']=round(all_img_df['typicality_score']*7/100,1)\n",
    "# all_img_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score=[]\n",
    "# for i in range(5):\n",
    "#     dir=all_img_df['dir'][i]\n",
    "#     score.append([all_img_df['typicality_score'][i], all_img_df['7_score'][i]])\n",
    "#     img=Image.open(dir)\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(score[i])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>dir</th>\n",
       "      <th>id_labels</th>\n",
       "      <th>readable_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>../Image/All_Cropped\\Mountain\\1.jpg</td>\n",
       "      <td>{'09332890-n': 0.33909276127815247, '09468604-...</td>\n",
       "      <td>{'lakeside': 0.33909276127815247, 'valley': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>../Image/All_Cropped\\Mountain\\10.jpg</td>\n",
       "      <td>{'09468604-n': 0.38023269176483154, '09193705-...</td>\n",
       "      <td>{'valley': 0.38023269176483154, 'alp': 0.21241...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>../Image/All_Cropped\\Mountain\\11.jpg</td>\n",
       "      <td>{'09193705-n': 0.6954638361930847, '09472597-n...</td>\n",
       "      <td>{'alp': 0.6954638361930847, 'volcano': 0.18501...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>../Image/All_Cropped\\Mountain\\12.jpg</td>\n",
       "      <td>{'09193705-n': 0.5897970795631409, '09468604-n...</td>\n",
       "      <td>{'alp': 0.5897970795631409, 'valley': 0.335895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>../Image/All_Cropped\\Mountain\\13.jpg</td>\n",
       "      <td>{'09246464-n': 0.49911436438560486, '09399592-...</td>\n",
       "      <td>{'cliff': 0.49911436438560486, 'promontory': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Church</td>\n",
       "      <td>../Image/All_Cropped\\Church\\5.jpg</td>\n",
       "      <td>{'03028079-n': 0.4907413423061371, '02825657-n...</td>\n",
       "      <td>{'church': 0.4907413423061371, 'bell cote': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Church</td>\n",
       "      <td>../Image/All_Cropped\\Church\\6.jpg</td>\n",
       "      <td>{'02793495-n': 0.9191867113113403, '03028079-n...</td>\n",
       "      <td>{'barn': 0.9191867113113403, 'church': 0.03191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Church</td>\n",
       "      <td>../Image/All_Cropped\\Church\\7.jpg</td>\n",
       "      <td>{'03781244-n': 0.6467804312705994, '03028079-n...</td>\n",
       "      <td>{'monastery': 0.6467804312705994, 'church': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Church</td>\n",
       "      <td>../Image/All_Cropped\\Church\\8.jpg</td>\n",
       "      <td>{'03028079-n': 0.4047333896160126, '02859443-n...</td>\n",
       "      <td>{'church': 0.4047333896160126, 'boathouse': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Church</td>\n",
       "      <td>../Image/All_Cropped\\Church\\9.jpg</td>\n",
       "      <td>{'03028079-n': 0.44589120149612427, '03781244-...</td>\n",
       "      <td>{'church': 0.44589120149612427, 'monastery': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                   dir  \\\n",
       "0    Mountain   ../Image/All_Cropped\\Mountain\\1.jpg   \n",
       "1    Mountain  ../Image/All_Cropped\\Mountain\\10.jpg   \n",
       "2    Mountain  ../Image/All_Cropped\\Mountain\\11.jpg   \n",
       "3    Mountain  ../Image/All_Cropped\\Mountain\\12.jpg   \n",
       "4    Mountain  ../Image/All_Cropped\\Mountain\\13.jpg   \n",
       "..        ...                                   ...   \n",
       "123    Church     ../Image/All_Cropped\\Church\\5.jpg   \n",
       "124    Church     ../Image/All_Cropped\\Church\\6.jpg   \n",
       "125    Church     ../Image/All_Cropped\\Church\\7.jpg   \n",
       "126    Church     ../Image/All_Cropped\\Church\\8.jpg   \n",
       "127    Church     ../Image/All_Cropped\\Church\\9.jpg   \n",
       "\n",
       "                                             id_labels  \\\n",
       "0    {'09332890-n': 0.33909276127815247, '09468604-...   \n",
       "1    {'09468604-n': 0.38023269176483154, '09193705-...   \n",
       "2    {'09193705-n': 0.6954638361930847, '09472597-n...   \n",
       "3    {'09193705-n': 0.5897970795631409, '09468604-n...   \n",
       "4    {'09246464-n': 0.49911436438560486, '09399592-...   \n",
       "..                                                 ...   \n",
       "123  {'03028079-n': 0.4907413423061371, '02825657-n...   \n",
       "124  {'02793495-n': 0.9191867113113403, '03028079-n...   \n",
       "125  {'03781244-n': 0.6467804312705994, '03028079-n...   \n",
       "126  {'03028079-n': 0.4047333896160126, '02859443-n...   \n",
       "127  {'03028079-n': 0.44589120149612427, '03781244-...   \n",
       "\n",
       "                                       readable_labels  \n",
       "0    {'lakeside': 0.33909276127815247, 'valley': 0....  \n",
       "1    {'valley': 0.38023269176483154, 'alp': 0.21241...  \n",
       "2    {'alp': 0.6954638361930847, 'volcano': 0.18501...  \n",
       "3    {'alp': 0.5897970795631409, 'valley': 0.335895...  \n",
       "4    {'cliff': 0.49911436438560486, 'promontory': 0...  \n",
       "..                                                 ...  \n",
       "123  {'church': 0.4907413423061371, 'bell cote': 0....  \n",
       "124  {'barn': 0.9191867113113403, 'church': 0.03191...  \n",
       "125  {'monastery': 0.6467804312705994, 'church': 0....  \n",
       "126  {'church': 0.4047333896160126, 'boathouse': 0....  \n",
       "127  {'church': 0.44589120149612427, 'monastery': 0...  \n",
       "\n",
       "[128 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_img_df.to_csv('vgg_scores_wordnet_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 1., 0., 2., 3., 3., 0., 2., 0., 3.]),\n",
       " array([21.9 , 27.58, 33.26, 38.94, 44.62, 50.3 , 55.98, 61.66, 67.34,\n",
       "        73.02, 78.7 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# mount=all_img_df[all_img_df['type']=='Beach']\n",
    "# plt.hist(mount['typicality_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
